{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is actually an open source framework, for extracting the data more easily and efficiently from websites. TO install this \n",
    "#in your system, run the command on terminal pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.1.0-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 26 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from scrapy) (2.8)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 314 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0-cp37-cp37m-macosx_10_6_intel.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 548 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from scrapy) (4.5.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from scrapy) (19.1.0)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.1.0-cp37-cp37m-macosx_10_9_x86_64.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 485 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 434 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 397 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-19.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 314 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (46.0.0.post20200309)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=d9e488a57fbeaaea30b94b316fb4a29dcc06705062b1bb34c3141dc820923a4c\n",
      "  Stored in directory: /Users/gaurishanand/Library/Caches/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=a308bfd16ff7c4ea518df5d75b928f241b0b5351ee0e43cc73d6c71b5b1263b2\n",
      "  Stored in directory: /Users/gaurishanand/Library/Caches/pip/wheels/dc/d0/bf/0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: queuelib, pyasn1, pyasn1-modules, service-identity, protego, hyperlink, zope.interface, incremental, constantly, PyHamcrest, Automat, Twisted, w3lib, cssselect, parsel, PyDispatcher, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.6.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.5.0 scrapy-2.1.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'myFirstProject', using template directory '/opt/anaconda3/lib/python3.7/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/gaurishanand/Desktop/Data Scientist CB/jupyter Notebook/WebScrapping/Web Crawler using Scrapy/myFirstProject\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd myFirstProject\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject myFirstProject #Command to create a new project in scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note after opening the project of scrapy,  create your spiderclass under spiders section only. You may refer to my project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
